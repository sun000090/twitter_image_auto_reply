{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVrX9rw80uOS",
    "outputId": "249d4f34-6f01-4055-b987-8efa00b079d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.4/94.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "pip -q install tweepy==4.10.0 openai==1.27.0 diffusers k-diffusion peft torchvision==0.16.0 git+https://github.com/sberbank-ai/Real-ESRGAN.git runpod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9g-k5EiAUoN",
    "outputId": "0cffdea4-df5d-4679-807d-8b0be48768c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated Text: ('Che bellissima giornat', 'What a beautiful day')\n"
     ]
    }
   ],
   "source": [
    "### OpenAI for translation\n",
    "import openai\n",
    "\n",
    "api_key = 'OPEN_AI_KEY'\n",
    "openai_client = openai.Client(api_key=api_key)\n",
    "\n",
    "def translate_text(text):\n",
    "    response = openai_client.chat.completions.create(\n",
    "              messages=[\n",
    "              {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Translate the given text to english. Text: {}\".format(text)\n",
    "              }\n",
    "              ],model=\"gpt-3.5-turbo\")\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "text_to_translate = \"Che bellissima giornat\"\n",
    "translated_text = translate_text(text_to_translate)\n",
    "print(f'Translated Text: {text_to_translate, translated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBMohIepAUlT",
    "outputId": "fd7a4924-291c-43c2-baca-c1daf4a624aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "### Stable Diffusion\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import peft\n",
    "from diffusers import StableDiffusionKDiffusionPipeline, StableDiffusionImg2ImgPipeline\n",
    "from RealESRGAN import RealESRGAN\n",
    "import os\n",
    "\n",
    "access_token = \"HUGGINGFACE_KEY\"\n",
    "\n",
    "def image_gen(prompts,device):\n",
    "    ### Load model file\n",
    "    pipe = StableDiffusionKDiffusionPipeline.from_pretrained('ai-chatbot-degen/yayoiMix_v25',\n",
    "                                                             torch_dtype=torch.float16,\n",
    "                                                             safety_checker=None,\n",
    "                                                            use_auth_token=access_token).to(device)\n",
    "\n",
    "    ### Scheduler or Sampler\n",
    "    pipe.set_scheduler('sample_dpmpp_2m_sde')\n",
    "\n",
    "    ### Negative prompt embedding\n",
    "    pipe.load_textual_inversion(\"ai-chatbot-degen/yayoiMix_v25\",\n",
    "                                        weight_name=\"easynegative.safetensors\",\n",
    "                                        token=\"easynegative\",\n",
    "                                        force_download=True)\n",
    "    pipe.load_textual_inversion(\"ai-chatbot-degen/yayoiMix_v25\",\n",
    "                                        weight_name=\"badhandv4.pt\",\n",
    "                                        token=\"badhand\",\n",
    "                                        force_download=True)\n",
    "    pipe.load_textual_inversion(\"ai-chatbot-degen/yayoiMix_v25\",\n",
    "                                        weight_name=\"negative_hand-neg.pt\",\n",
    "                                        token=\"negativehand\",\n",
    "                                        force_download=True)\n",
    "    pipe.load_textual_inversion(\"ai-chatbot-degen/yayoiMix_v25\",\n",
    "                                        weight_name=\"bad-hands-5.pt\",\n",
    "                                        token=\"badhand5\",\n",
    "                                        force_download=True)\n",
    "\n",
    "    # Hyperparamters\n",
    "    num_images_per_prompt = 1\n",
    "    height = 880\n",
    "    width = 544\n",
    "\n",
    "    # Seed parameter\n",
    "    generator=torch.Generator(device=device)\n",
    "    seeds = []\n",
    "\n",
    "    for _ in range(num_images_per_prompt):\n",
    "        seed = generator.seed()\n",
    "        seeds.append(seed)\n",
    "        generator = generator.manual_seed(seed)\n",
    "\n",
    "    prompts = prompts\n",
    "    negative_prompts = '''\n",
    "        Negative prompt: painting,sketches,(worst quality:2),(low quality:2),(normal quality:2),((monochrome)),\n",
    "        ((grayscale)), missing fingers ,skin spots ,acnes,skin blemishes,loli, easynegative, Bad-Hands-5, negative_hand\n",
    "        '''\n",
    "    num_inference_steps = 33\n",
    "    guidance_scale = 7\n",
    "    clip_skip = 1\n",
    "    ### Image generation\n",
    "    out = pipe(prompts,\n",
    "        generator=generator,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_images_per_prompt=1,\n",
    "        negative_prompt=negative_prompts,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        clip_skip=clip_skip,\n",
    "        use_karras_sigmas=True).images\n",
    "\n",
    "    ### Denoising\n",
    "    img2img = StableDiffusionImg2ImgPipeline(**pipe.components)\n",
    "    res_image = img2img(prompt=prompts,\n",
    "        negative_prompt=negative_prompts,\n",
    "        generator=generator,\n",
    "        num_inference_steps=num_inference_steps*3,\n",
    "        image = out[0].resize((width*1, height*1)),\n",
    "        strength = 0.45,\n",
    "        guidance_scale=guidance_scale,\n",
    "        clip_skip=clip_skip\n",
    "        ).images\n",
    "\n",
    "    ### Upscaler\n",
    "    model1 = RealESRGAN(device=\"cuda\", scale=8)\n",
    "    model1.load_weights('weights/RealESRGAN_x8.pth', download=True)\n",
    "    sr_image = model1.predict(res_image[0])\n",
    "\n",
    "    return sr_image.resize((320,520))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGuS_cmcRTPz",
    "outputId": "beaffdcc-1be6-4221-b0a1-ea7bd6074e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='HUGGINGFACE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQRLDygoAUin"
   },
   "outputs": [],
   "source": [
    "### Runpod\n",
    "import runpod\n",
    "\n",
    "runpod.api_key = \"RUNPOD_KEY\"\n",
    "endpoints = runpod.get_endpoints()\n",
    "# print(endpoints)\n",
    "\n",
    "endpoint = runpod.Endpoint(\"ENDPOINTS\")\n",
    "output = image_gen('Hi world','cuda')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R70bYm2Zat57"
   },
   "outputs": [],
   "source": [
    "API_KEY = 'API'\n",
    "API_KEY_SECRET = 'SECRET'\n",
    "ACCESS_TOKEN = 'TOKEN'\n",
    "ACCESS_TOKEN_SECRET = 'TOEKN_SECRET'\n",
    "BEARER_TOKEN = 'BEARER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Szt3T7UL-wbj"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import os\n",
    "\n",
    "consumer_key = API_KEY\n",
    "consumer_secret = API_KEY_SECRET\n",
    "access_token = ACCESS_TOKEN\n",
    "access_token_secret = ACCESS_TOKEN_SECRET\n",
    "bearer_token = BEARER_TOKEN\n",
    "\n",
    "auth = tweepy.OAuth1UserHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth=auth, wait_on_rate_limit=True)\n",
    "client = tweepy.Client(bearer_token, consumer_key, consumer_secret, access_token, access_token_secret, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "puzhj-oS_GRi"
   },
   "outputs": [],
   "source": [
    "### Get own user account\n",
    "client_id = client.get_me().data.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-R9PgFUD8YS"
   },
   "outputs": [],
   "source": [
    "### Fetch latest tweets\n",
    "start_id = 1\n",
    "initialisation_resp = client.get_users_mentions(client_id)\n",
    "if initialisation_resp.data != None:\n",
    "    start_id = initialisation_resp.data[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "McOg8eHxEGSa"
   },
   "outputs": [],
   "source": [
    "### Automated replies\n",
    "while True:\n",
    "    response = client.get_users_mentions(client_id, since_id=start_id)\n",
    "\n",
    "    # Reply Code\n",
    "    if response.data != None:\n",
    "        for tweet in response.data:\n",
    "            try:\n",
    "                message = image_gen(translate_text(tweet.text),'cuda')\n",
    "                # client.create_tweet(in_reply_to_tweet_id=tweet.id, text='Follow me for more')\n",
    "                api.update_with_media(filename = message, status = 'Follow me for more', in_reply_to_status_id = tweet.id)\n",
    "                start_id = tweet.id\n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "\n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZRhkKneFHgK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
